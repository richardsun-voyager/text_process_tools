{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data usually consists of a series of unstructured symbols like words, punctuation, digits all of which combine into strings with varied lengths. It is complex but also necessary to handle text data ahead of NLP tasks such as feature extraction, classification and etc. In order to facilitate text processing, I wrote four tools to handle text data(English):\n",
    "- text_preprocess.py : Remove punctuation and digits, lemmatize words and return clean text strings\n",
    "- text_vectorizer.py : Simply clean the texts, vectorize them using tools from scikit-learn\n",
    "- text_hier_split: Split texts into hierarchical structure, for example, a text can be divided into several sentences, and each sentence can be devided into tokens like words and punctuation\n",
    "- token_idx_map: Build a vacabulary and a dictionary for texts of hierarchical-structure, map each word into an unique ID for latter usage like deep learning applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we take 20newsgroup data as example to do text cleaning and subsequent tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch the news texts\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                  shuffle=True, random_state=11)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                  shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training texts: 11314\n",
      "Number of testing texts: 7532\n"
     ]
    }
   ],
   "source": [
    "print('Number of training texts:', len(newsgroups_train.data))\n",
    "print('Number of testing texts:', len(newsgroups_test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the news. It seems quite messy, that's why we need preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: bear@kestrel.fsl.noaa.gov (Bear Giles)\n",
      "Subject: Re: Fifth Amendment and Passwords\n",
      "Organization: Forecast Systems Labs, NOAA, Boulder, CO USA\n",
      "Lines: 37\n",
      "\n",
      "In article <1993Apr15.160415.8559@magnus.acs.ohio-state.edu> ashall@magnus.acs.ohio-state.edu (Andrew S Hall) writes:\n",
      ">I am postive someone will correct me if I am wrong, but doesn't the Fifth\n",
      ">also cover not being forced to do actions that are self-incriminating?\n",
      ">e.g. The police couldn't demand that you silently take them to where the\n",
      ">body is buried or where the money is hidden.\n",
      "\n",
      "But they can make you piss in a jar, and possibly provide DNA, semen,\n",
      "and hair samples or to undergo tests for gunpowder residues on your hand.\n",
      "\n",
      "(BTW, that was why the chemical engineer arrested in the WTC explosion\n",
      "thrust his hands into a toilet filled with urine as the cops were breaking\n",
      "down the door -- the nitrogen in the urine would mask any residue from\n",
      "explosives.  I found it interesting the news reported his acts, but not\n",
      "his reasons).\n",
      "\n",
      "Somewhere, perhaps a privacy group, they discussed the legal ramifications\n",
      "of using a password like\n",
      "\n",
      "  I shot Jimmy Hoffa and his body is in a storage locker in Camden\n",
      "\n",
      "a while back.  The impression I got was that real judges would dismiss\n",
      "arguments that this password is self-incrimination as first-year law\n",
      "school sophistry -- the fact that you use a statement for a password has\n",
      "no bearing on the veracity of that phrase.\n",
      "\n",
      "You are not being asked to incrimidate yourself (e.g., \"where did you\n",
      "bury the body?\"); you are being asked to provide information necessary\n",
      "to execute a legal search warrant.  Refusing to provide the password is\n",
      "akin to refusing to provide a key to a storage locker... except that they\n",
      "could always _force_ their way into the locker.\n",
      "\n",
      "Of course, that doesn't mean you have to help them _understand_ what\n",
      "they find, or point out things they overlooked in their search!\n",
      "\n",
      "-- \n",
      "Bear Giles\n",
      "bear@fsl.noaa.gov\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean texts and Lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = newsgroups_train.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to process....\n",
      "Processing Finished! Timing:  25.959\n"
     ]
    }
   ],
   "source": [
    "from text_preprocess import text_clean\n",
    "tp = text_clean(texts)\n",
    "processed_texts = tp.proceed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from bear kestrel fsl noaa gov bear giles subject re fifth amendment and password organization forecast system lab noaa boulder co usa line in article apr magnus ac ohio state edu ashall magnus ac ohio state edu andrew s hall writes i am postive someone will correct me if i am wrong but doesn t the fifth also cover not being forced to do action that are self incriminating e g the police couldn t demand that you silently take them to where the body is buried or where the money is hidden but they can make you piss in a jar and possibly provide dna semen and hair sample or to undergo test for gunpowder residue on your hand btw that wa why the chemical engineer arrested in the wtc explosion thrust his hand into a toilet filled with urine a the cop were breaking down the door the nitrogen in the urine would mask any residue from explosive i found it interesting the news reported his act but not his reason somewhere perhaps a privacy group they discussed the legal ramification of using a password like i shot jimmy hoffa and his body is in a storage locker in camden a while back the impression i got wa that real judge would dismiss argument that this password is self incrimination a first year law school sophistry the fact that you use a statement for a password ha no bearing on the veracity of that phrase you are not being asked to incrimidate yourself e g where did you bury the body you are being asked to provide information necessary to execute a legal search warrant refusing to provide the password is akin to refusing to provide a key to a storage locker except that they could always force their way into the locker of course that doesn t mean you have to help them understand what they find or point out thing they overlooked in their search bear giles bear fsl noaa gov\n"
     ]
    }
   ],
   "source": [
    "print(processed_texts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything other than words are gone, and words of plural forms like 'lines' have been transformed into original form 'line', but do be cautious, sometimes the symbols like punctuation can have some impact in terms of semantics, for example, in sentiment analysis, symbols like '?', '!' are also meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Text Data\n",
    "\n",
    "In a classical machine learning task, we need to transform texts into vectors of fixed length as the input of subsequent algorithms. There are many methods to vectorize texts, for example, we can build a vocabulary, each word stands for a feature, connsequently it is convenient represent a text as a vector of frequences of words in the vocabulary. Alternatively, we can use TfIdf values to replace simple frequences in the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_vectorizer import text_vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tv = text_vectorizer(texts, vectorizer=CountVectorizer())\n",
    "vecs = tv.proceed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 82810)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each text is represented as a vector with 82910 features, below are frequences for each word that do appear in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2556)\t1\n",
      "  (0, 32967)\t1\n",
      "  (0, 75210)\t1\n",
      "  (0, 42322)\t1\n",
      "  (0, 33339)\t1\n",
      "  (0, 75844)\t2\n",
      "  (0, 58932)\t1\n",
      "  (0, 39737)\t2\n",
      "  (0, 22607)\t2\n",
      "  (0, 80309)\t2\n",
      "  (0, 71616)\t1\n",
      "  (0, 19936)\t3\n",
      "  (0, 45221)\t3\n",
      "  (0, 21166)\t1\n",
      "  (0, 31029)\t1\n",
      "  (0, 54972)\t1\n",
      "  (0, 49041)\t1\n",
      "  (0, 40646)\t1\n",
      "  (0, 34169)\t2\n",
      "  (0, 70699)\t1\n",
      "  (0, 51173)\t1\n",
      "  (0, 29761)\t2\n",
      "  (0, 68298)\t1\n",
      "  (0, 63505)\t1\n",
      "  (0, 21555)\t1\n",
      "  (0, 44002)\t1\n",
      "  (0, 19979)\t2\n",
      "  (0, 19978)\t2\n",
      "  (0, 25354)\t2\n"
     ]
    }
   ],
   "source": [
    "print(vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Text into Hiararchical Structure\n",
    "\n",
    "Texts vary much, however in terms of structure, we can view that an article consists of paragraphs, a paragraph consists of sentences, and a sentence consists of words. To simplify, we can also represent a text as a sequence of sentences, and each sentences as a sequence of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tokenizing....\n",
      "Processing Finished! Timing:  26.928\n",
      "[['from', ':', 'email', '(', ')', 'subject', ':', 'help', 'organization', ':', 'the', 'internet', 'line', ':', 'nntp', 'posting', 'host', ':', 'enterpoop', '.', 'mit', '.', 'edu', 'to', ':', 'email', 'received', 'from', 'eei', '.', 'eeiihy', '.'], ['vax', '.', 'xpert', '..', 'expo', '.', 'lcs', '.', 'mit', '.', 'edu', '..', 'inet', ':', 'mail', 'user', 'in', 'vax', 'and', 'internet', 'help']]\n"
     ]
    }
   ],
   "source": [
    "from text_hier_split import text2sents, sent2words\n",
    "ts = text2sents(texts)\n",
    "sents = ts.proceed()\n",
    "print(sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also adopt textBlob package here, it works pretty fast but not so flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing: 0.489\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from time import time\n",
    "start = time()\n",
    "blobs = list(map(TextBlob, texts))\n",
    "end = time()\n",
    "print('Timing:', round(end-start, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"From: bear@kestrel.fsl.noaa.gov (Bear Giles)\n",
       " Subject: Re: Fifth Amendment and Passwords\n",
       " Organization: Forecast Systems Labs, NOAA, Boulder, CO USA\n",
       " Lines: 37\n",
       " \n",
       " In article <1993Apr15.160415.8559@magnus.acs.ohio-state.edu> ashall@magnus.acs.ohio-state.edu (Andrew S Hall) writes:\n",
       " >I am postive someone will correct me if I am wrong, but doesn't the Fifth\n",
       " >also cover not being forced to do actions that are self-incriminating?\"),\n",
       " Sentence(\">e.g.\"),\n",
       " Sentence(\"The police couldn't demand that you silently take them to where the\n",
       " >body is buried or where the money is hidden.\"),\n",
       " Sentence(\"But they can make you piss in a jar, and possibly provide DNA, semen,\n",
       " and hair samples or to undergo tests for gunpowder residues on your hand.\"),\n",
       " Sentence(\"(BTW, that was why the chemical engineer arrested in the WTC explosion\n",
       " thrust his hands into a toilet filled with urine as the cops were breaking\n",
       " down the door -- the nitrogen in the urine would mask any residue from\n",
       " explosives.\"),\n",
       " Sentence(\"I found it interesting the news reported his acts, but not\n",
       " his reasons).\"),\n",
       " Sentence(\"Somewhere, perhaps a privacy group, they discussed the legal ramifications\n",
       " of using a password like\n",
       " \n",
       "   I shot Jimmy Hoffa and his body is in a storage locker in Camden\n",
       " \n",
       " a while back.\"),\n",
       " Sentence(\"The impression I got was that real judges would dismiss\n",
       " arguments that this password is self-incrimination as first-year law\n",
       " school sophistry -- the fact that you use a statement for a password has\n",
       " no bearing on the veracity of that phrase.\"),\n",
       " Sentence(\"You are not being asked to incrimidate yourself (e.g., \"where did you\n",
       " bury the body?\"),\n",
       " Sentence(\"\"); you are being asked to provide information necessary\n",
       " to execute a legal search warrant.\"),\n",
       " Sentence(\"Refusing to provide the password is\n",
       " akin to refusing to provide a key to a storage locker... except that they\n",
       " could always _force_ their way into the locker.\"),\n",
       " Sentence(\"Of course, that doesn't mean you have to help them _understand_ what\n",
       " they find, or point out things they overlooked in their search!\"),\n",
       " Sentence(\"-- \n",
       " Bear Giles\n",
       " bear@fsl.noaa.gov\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blobs[2].sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['From', 'bear', 'kestrel.fsl.noaa.gov', 'Bear', 'Giles', 'Subject', 'Re', 'Fifth', 'Amendment', 'and', 'Passwords', 'Organization', 'Forecast', 'Systems', 'Labs', 'NOAA', 'Boulder', 'CO', 'USA', 'Lines'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blobs[2].sentences[0].words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Text into Sequences of IDs\n",
    "In many machine learning tasks, in order to facilitate the operations, we need to convert word symbols to IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start mapping words to IDs....\n",
      "Processing Finished! Timing:  0.002\n"
     ]
    }
   ],
   "source": [
    "from token_idx_map import token2idx\n",
    "ti = token2idx(sents[:5])\n",
    "sent_idx = ti.proceed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[29,\n",
       "  9,\n",
       "  16,\n",
       "  13,\n",
       "  15,\n",
       "  56,\n",
       "  9,\n",
       "  119,\n",
       "  63,\n",
       "  9,\n",
       "  1,\n",
       "  294,\n",
       "  77,\n",
       "  9,\n",
       "  273,\n",
       "  185,\n",
       "  193,\n",
       "  9,\n",
       "  656,\n",
       "  0,\n",
       "  263,\n",
       "  0,\n",
       "  282,\n",
       "  4,\n",
       "  9,\n",
       "  16,\n",
       "  394,\n",
       "  29,\n",
       "  449,\n",
       "  0,\n",
       "  337,\n",
       "  0],\n",
       " [199,\n",
       "  0,\n",
       "  384,\n",
       "  224,\n",
       "  623,\n",
       "  0,\n",
       "  798,\n",
       "  0,\n",
       "  263,\n",
       "  0,\n",
       "  282,\n",
       "  224,\n",
       "  348,\n",
       "  9,\n",
       "  805,\n",
       "  525,\n",
       "  10,\n",
       "  199,\n",
       "  12,\n",
       "  294,\n",
       "  119]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start mapping words to IDs....\n",
      "Processing Finished! Timing:  0.029\n"
     ]
    }
   ],
   "source": [
    "ti = token2idx(blobs[:5])\n",
    "sent_idx = ti.proceed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46,\n",
       " 236,\n",
       " 901,\n",
       " 186,\n",
       " 54,\n",
       " 111,\n",
       " 66,\n",
       " 35,\n",
       " 799,\n",
       " 72,\n",
       " 98,\n",
       " 281,\n",
       " 346,\n",
       " 344,\n",
       " 747,\n",
       " 784,\n",
       " 854,\n",
       " 57,\n",
       " 236,\n",
       " 186,\n",
       " 341,\n",
       " 581,\n",
       " 446,\n",
       " 389,\n",
       " 859,\n",
       " 589,\n",
       " 774,\n",
       " 745,\n",
       " 810,\n",
       " 111]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
